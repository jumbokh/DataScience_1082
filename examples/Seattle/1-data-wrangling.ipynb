{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling of Fremont Bridge's data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Fremont Bridge's data into local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as req\n",
    "# req.urlretrieve('https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD', 'data/FremontBridge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and  start exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fb.head(5))\n",
    "print(fb.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.columns = ['East','West']\n",
    "fb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb['Total'] = fb['East'] + fb['West']\n",
    "fb['Total'] = fb.eval('East + West')\n",
    "fb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "fb.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.dropna(inplace=True)\n",
    "fb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.plot(kind='line', figsize=(16,6))\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Hourly Bicycle Count')\n",
    "# Obsevation: The 30k+ samples are way too dense to make sense, because we hardly see East and West data ata the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about if we resample the data from hourly to daily?\n",
    "fb_daily = fb.resample('d').sum()\n",
    "print('fb_daily size :',fb_daily.size)\n",
    "fb_daily.plot(kind='line', figsize=(16,6))\n",
    "plt.ylabel('Daily Bicycle Count')\n",
    "# Observation: It's still dense with daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about if we resample the data from daily to weekly? 53 weeks a year isn't much dense after  all, right?\n",
    "fb_weekly = fb_daily.resample('W').sum()\n",
    "print('fb_weekly size :',fb_weekly.size)\n",
    "fb_weekly.plot(kind='line', figsize=(16,6))\n",
    "plt.ylabel('Weekly Bicycle Count')\n",
    "# Observation: Life is better now..for data analysis\n",
    "# The frequency line varies widely. What do we infer?\n",
    "# People bicycle more in summer (see the peaks evevry year in the graph) than in winter\n",
    "# Even within a season, there are wide-fluctuations than near-consistency in the counts from one week to another. Why?\n",
    "# Perhaps because of temperature, precipitation and other factors.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same weekly  plot can be pllot from daily dataset like below\n",
    "fb_daily.rolling(window=7).mean().plot(kind='line', figsize=(16,6))\n",
    "plt.title('Aggregating the data using a weekly(window = 7 days)  rolling mean')\n",
    "plt.ylabel('Weekly Bicycle Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The jaggedness of the result is due to the hard cutoff of the window. \n",
    "# We can get a smoother version of a rolling mean using a window function—for example, a Gaussian window.\n",
    "# The above weekly plot is a little jagged and can be smoothened like below:\n",
    "# The following code specifies both the width of the window (we chose 7 days) and \n",
    "# the width of the Gaussian within the window (we chose 3 days):\n",
    "fb_daily.rolling(window=7, center=True, win_type='gaussian').mean(std=5).plot(figsize=(16,6))\n",
    "plt.title('Aggregating the data using a weekly(window = 7 days)  rolling mean')\n",
    "plt.ylabel('Weekly Bicycle Count')\n",
    "plt.show()\n",
    "# Observation: Smoothening isn't really so good in this case, likee how it used to be :-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to look at the average traffic as a function of the time of day.\n",
    "gby_time = fb.groupby(fb.index.time).mean()\n",
    "hourly_ticks = 60 * 60 * np.arange(24)\n",
    "print('Hourly Ticks :', hourly_ticks)\n",
    "gby_time.plot(xticks=hourly_ticks,figsize=(12,6))\n",
    "plt.xticks(rotation='vertical')\n",
    "# Observation: The hourly traffic is a strongly bimodal distribution (do yu see 2 mountains in the graph?), \n",
    "# with peaks around 8:00 in the morning and 5:00 in the evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We also might be curious about how things change based on the day of the week. \n",
    "# Again, we can do this with a simple groupby\n",
    "gby_week = fb.groupby(fb.index.dayofweek).mean()\n",
    "# print(gby_week)\n",
    "weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "# print('gby_week index =',gby_week.index.tolist())\n",
    "# gby_week.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "# print('gby_week index =',gby_week.index.tolist())\n",
    "gby_week.plot()\n",
    "plt.xticks(range(len(weekdays)),weekdays, rotation=45)\n",
    "plt.show()\n",
    "# Observation: This shows a strong distinction between weekday and weekend totals, \n",
    "# with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_mask = np.where(fb_daily.index.weekday < 5, 'Weekday', 'Weekend') # 5 and 6 are Sat and Sun => Weekend\n",
    "len(weekday_mask)\n",
    "weekday_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the hourly trend looks like between weekdays and weekends\n",
    "# Now that  means we arer grouping-by weekday-mask and hourly-time\n",
    "weekday_mask = np.where(fb.index.weekday < 5, 'weekday','weekend')\n",
    "gby_hourly = fb.groupby([weekday_mask,fb.index.time]).mean()\n",
    "gby_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_ticks = 60 * 60 * np.arange(24)\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "gby_hourly.loc['weekday',:].plot(ax=ax[0], xticks=hourly_ticks, title='Weekdays', style=[':','--','-.'])\n",
    "gby_hourly.loc['weekend',:].plot(ax=ax[1], xticks=hourly_ticks, title='Weekends', style=[':','--','-.'])\n",
    "\n",
    "\n",
    "# plt.xticks(rotation=90) only affects the last subplot whose reference is active\n",
    "# To change the xticks on every sub-plot\n",
    "for x in ax:\n",
    "    x.xaxis.set_tick_params(rotation=90)\n",
    "    \n",
    "# Observation: We see a bimodal commute pattern during the work week, and a unimodal recreational pattern during the weekends.\n",
    "# It would be interesting to dig through this data in more detail, and examine the effect of weather, temperature, time of year, and other factors on people’s commuting patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling by merging Datasets (Fremont Bridge and Weather Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Daily Weather data of Fremont Bridge\n",
    "wd = pd.read_csv('data/1404899.csv', index_col='DATE', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_daily.shape # The number of days/rows match in both datasets - fb_daily and wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd[['STATION','NAME','PRCP','SNOW','TMAX','TMIN']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = wd.filter(['STATION','NAME','PRCP','SNOW','TMAX','TMIN'], axis=1)\n",
    "wd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Snowy days are recorded? how do the values look like?\n",
    "np.sort( wd.SNOW.unique() ) # So the values range between 0 and 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort( wd.PRCP.unique() ) # So the values range between 0 and 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.loc[ (wd['SNOW'] > 0) & (wd['PRCP'] == 0) ] # No rows available. So, the precipitation (PRCP) cannot be 0, when it Snows (SNOW > 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fb_daily['Total'].copy()\n",
    "df = fb_daily.filter(['Total'], axis=1)\n",
    "print('Shape : ',df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering : Categorical Values\n",
    "* We saw previously that the patterns of use generally vary from day to day; let’s account for this in our data by adding binary columns that indicate the day of the week.\n",
    "* Similarly, we might expect riders to behave differently on holidays; let’s add an indicator of this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekdays)\n",
    "print(list(range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature  Engineering : Categorical Values : adding binary columns that indicate the day of the week\n",
    "for i in range(7):\n",
    "    df[weekdays[i]] = (df.index.weekday ==  i).astype(int)\n",
    "df.head(7)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns['Total'] = df.columns['TotalBikesCount']\n",
    "df = df.rename(columns={'Total':'TotalBikesCount'})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming, riders to behave differently on holidays.; we add an indicator for this by incorporating National Holiday Calendar:\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar # Can't believe panda has a class for this!\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2018')\n",
    "df = df.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "df['holiday'].fillna(0, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['holiday'], axis=1, inplace=True)\n",
    "df['holiday'] = df['holiday'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['holiday'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for the Duration of Daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also suspect that the hours of daylight would affect how many people ride; \n",
    "# let’s use the standard astronomical calculation to add this information\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - pd.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude)) * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daylight_hrs'] = list(map(hours_of_daylight, df.index))\n",
    "df[['daylight_hrs']].plot(fig=(16,4))\n",
    "plt.ylim(8,17) # Increased y-axis limit max range by 1. Helps in placing the legend neatly without overlapping on the curve\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Hours of daylight in Seattle')\n",
    "plt.show()\n",
    "# Observation: The daylight varies between ~8 hrs (during Dececmber) and ~16 hrs (June)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does day-light duration affect bicycle traffic? Let's validate that assumption with scatter plot.\n",
    "\n",
    "# Plotting with Seaborn\n",
    "# sns.lmplot('daylight_hrs','TotalBikesCount',data=df, fit_reg=False, size=5, scatter=True)\n",
    "\n",
    "# plotting with Matplotlib\n",
    "# plt.plot(df['daylight_hrs'],df['TotalBikesCount'])\n",
    "plt.figure(figsize=(12,6)) # This should be set before drawing the plot\n",
    "plt.scatter(df['daylight_hrs'],df['TotalBikesCount'])\n",
    "plt.show()\n",
    "# While see that our assumption turning out to be truthy, we see the plot too dense with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the plot with resampled data\n",
    "weekly = df.resample('W').mean()\n",
    "# weekly.head()\n",
    "plt.scatter(weekly['daylight_hrs'],weekly['TotalBikesCount'])\n",
    "# So this clears the density to a large extent and we can see the trruthiness of our assumption - More daylight hours, more traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = df.sample(n=500)\n",
    "sample = df.sample(frac=0.1) # 10% of data ffrom the df\n",
    "plt.scatter(weekly['daylight_hrs'],weekly['TotalBikesCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we see how the change in the duration affects the cycle traffic quantitatively?\n",
    "# We can with LR's coefficient\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = sample[['daylight_hrs']]\n",
    "y = sample['TotalBikesCount']\n",
    "clf = LinearRegression(fit_intercept=True).fit(x, y)\n",
    "ypred = clf.predict(x)\n",
    "print(clf.coef_) # OP: ~295\n",
    "# Now that means on any given day, each extra hour of daylight leads to about 300 more cyclers using the bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now be a data scientist and ask yourself?\n",
    "\n",
    "So is the seasonal increase daylight duration the real factor for the rise in bicyclers?\n",
    "Or do we actually see a trend in the rise in bicyclers?\n",
    "The way we can find answers to this is by doing a de-trending on the data. \n",
    "\n",
    "This is what I mean by \"de-trended\" data. We've basically removed the component of the data which correlates with the number of hours in a day, so that what is left is in some way agnostic to this quantity. The \"adjusted weekly count\" plotted here can be thought of as the number of cyclists we'd expect to see if the hours of daylight were not a factor.\n",
    "\n",
    "**With the data de-trended, we get a better idea of how bicycling in Seattle has changed over time, corrected for the seasonal variation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have fit this trend, let's subtract it off and replace it by the mean:\n",
    "trend = clf.predict(weekly[['daylight_hrs']]) # Trend is the weekly-predictions\n",
    "detrended = trend.mean() + (weekly['TotalBikesCount'] - trend )\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(weekly['daylight_hrs'],detrended)\n",
    "plt.title('Weekly De-trended Traffic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this another way. in 2 steps.\n",
    "# Step 1: Instead of plotting the number of riders vs daylight hours, \n",
    "# we'll again plot the number of riders vs the day of the year, along with the trend\n",
    "weekly['daylight_trend'] = trend\n",
    "weekly[['TotalBikesCount','daylight_trend']].plot(figsize=(14,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: We can similarly view the adjusted total number of riders over time by subtracting this green line from the blue line:\n",
    "detrended.plot(figsize=(14,7))\n",
    "print('The STD of the detrended cyclists is {0:.0f}'.format(detrended.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding average temperature\n",
    "# # temperatures are in 1/10 deg C; convert to C\n",
    "tmin = wd['TMIN'] / 10\n",
    "tmax = wd['TMAX'] / 10\n",
    "wd['AvgTempInC'] = (tmin + tmax) / 2\n",
    "df = df.join(wd['AvgTempInC'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip is in 1/10 mm; convert to inches\n",
    "wd['PRCP_IN'] = wd['PRCP'] / 254\n",
    "wd['DryDay'] = (wd['PRCP'] == 0).astype(int)\n",
    "df = df.join(wd[['PRCP_IN','DryDay']])\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'daylight_hrs':'DayLightHrs'})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let’s add a counter that increases from day 1, and measures how many years have passed. \n",
    "# This will let us measure any observed annual increase or decrease in daily crossings:\n",
    "df['YearsCount'] = (df.index - df.index[0]).days / 365.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
